{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.922757\tvalid_1's auc: 0.878045\n",
      "[500]\ttraining's auc: 0.947055\tvalid_1's auc: 0.896394\n",
      "[750]\ttraining's auc: 0.954249\tvalid_1's auc: 0.905641\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's auc: 0.954249\tvalid_1's auc: 0.905641\n",
      "0 MEAN:  0.9056410690718667 LAST:  0.9056410690718667\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.921975\tvalid_1's auc: 0.853816\n",
      "[500]\ttraining's auc: 0.947294\tvalid_1's auc: 0.872296\n",
      "[750]\ttraining's auc: 0.957418\tvalid_1's auc: 0.879047\n",
      "Early stopping, best iteration is:\n",
      "[731]\ttraining's auc: 0.957418\tvalid_1's auc: 0.879047\n",
      "1 MEAN:  0.8923438349106871 LAST:  0.8790466007495077\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.919841\tvalid_1's auc: 0.895957\n",
      "[500]\ttraining's auc: 0.94466\tvalid_1's auc: 0.913105\n",
      "[750]\ttraining's auc: 0.954103\tvalid_1's auc: 0.919341\n",
      "Early stopping, best iteration is:\n",
      "[692]\ttraining's auc: 0.954103\tvalid_1's auc: 0.919341\n",
      "2 MEAN:  0.901342750406395 LAST:  0.9193405813978107\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.920163\tvalid_1's auc: 0.868844\n",
      "[500]\ttraining's auc: 0.94616\tvalid_1's auc: 0.891008\n",
      "[750]\ttraining's auc: 0.95594\tvalid_1's auc: 0.897296\n",
      "Early stopping, best iteration is:\n",
      "[693]\ttraining's auc: 0.955927\tvalid_1's auc: 0.897415\n",
      "3 MEAN:  0.9003607270648515 LAST:  0.8974146570402207\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.923435\tvalid_1's auc: 0.871256\n",
      "[500]\ttraining's auc: 0.947301\tvalid_1's auc: 0.886274\n",
      "[750]\ttraining's auc: 0.95735\tvalid_1's auc: 0.895175\n",
      "Early stopping, best iteration is:\n",
      "[749]\ttraining's auc: 0.95735\tvalid_1's auc: 0.895175\n",
      "4 MEAN:  0.8993235375071457 LAST:  0.8951747792763227\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.921387\tvalid_1's auc: 0.857702\n",
      "[500]\ttraining's auc: 0.946528\tvalid_1's auc: 0.872163\n",
      "[750]\ttraining's auc: 0.954929\tvalid_1's auc: 0.878275\n",
      "Early stopping, best iteration is:\n",
      "[694]\ttraining's auc: 0.954902\tvalid_1's auc: 0.87828\n",
      "5 MEAN:  0.8958162776801487 LAST:  0.8782799785451645\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.919253\tvalid_1's auc: 0.892879\n",
      "[500]\ttraining's auc: 0.94331\tvalid_1's auc: 0.908788\n",
      "[750]\ttraining's auc: 0.95305\tvalid_1's auc: 0.91544\n",
      "Early stopping, best iteration is:\n",
      "[682]\ttraining's auc: 0.95305\tvalid_1's auc: 0.91544\n",
      "6 MEAN:  0.898619727156599 LAST:  0.9154404240153007\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.918913\tvalid_1's auc: 0.902475\n",
      "[500]\ttraining's auc: 0.945069\tvalid_1's auc: 0.911236\n",
      "[750]\ttraining's auc: 0.957536\tvalid_1's auc: 0.915393\n",
      "Early stopping, best iteration is:\n",
      "[780]\ttraining's auc: 0.958016\tvalid_1's auc: 0.916428\n",
      "7 MEAN:  0.9008366680075939 LAST:  0.9163552539645572\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.920062\tvalid_1's auc: 0.88472\n",
      "[500]\ttraining's auc: 0.945017\tvalid_1's auc: 0.903854\n",
      "[750]\ttraining's auc: 0.95595\tvalid_1's auc: 0.910274\n",
      "Early stopping, best iteration is:\n",
      "[710]\ttraining's auc: 0.955461\tvalid_1's auc: 0.91028\n",
      "8 MEAN:  0.9018858847414245 LAST:  0.9102796186120697\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.923603\tvalid_1's auc: 0.855462\n",
      "[500]\ttraining's auc: 0.946733\tvalid_1's auc: 0.881942\n",
      "[750]\ttraining's auc: 0.956012\tvalid_1's auc: 0.892498\n",
      "Early stopping, best iteration is:\n",
      "[727]\ttraining's auc: 0.956012\tvalid_1's auc: 0.892498\n",
      "9 MEAN:  0.9009470923346014 LAST:  0.8924979606731925\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "train=pd.read_csv('train.csv')\n",
    "campaign_data=pd.read_csv('campaign_data.csv')\n",
    "coupon_item_mapping=pd.read_csv('coupon_item_mapping.csv')\n",
    "customer_demographics=pd.read_csv('customer_demographics.csv')\n",
    "customer_transaction_data=pd.read_csv('customer_transaction_data.csv')\n",
    "item_data=pd.read_csv('item_data.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('submission.csv')\n",
    "\n",
    "data = pd.concat([train, test], sort=False).reset_index(drop = True)\n",
    "ltr = len(train)\n",
    "data = data.merge(campaign_data, on='campaign_id')#  campaign_data\n",
    "data = data.merge(customer_demographics, on='customer_id',how='left') #  customer_demographics\n",
    "data = pd.merge_asof(data.sort_values(\"customer_id\"), customer_transaction_data.sort_values(\"customer_id\"), on='customer_id')\n",
    "data = pd.merge_asof(data.sort_values(\"item_id\"), item_data.sort_values(\"item_id\"), on='item_id')\n",
    "data['start_date'] = pd.to_datetime(data['start_date'])\n",
    "data['end_date'] = pd.to_datetime(data['end_date'])\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "\n",
    "for i in data.columns:\n",
    "    if str(data[i].dtype) == 'object':\n",
    "        data[i] = data[i].factorize()[0]\n",
    "\n",
    "train_cols = ['campaign_id', 'coupon_id', 'customer_id',\n",
    "       'campaign_type', 'age_range','marital_status', 'rented', 'family_size', 'no_of_children',\n",
    "       'income_bracket', 'item_id', 'quantity', 'selling_price',\n",
    "       'other_discount', 'coupon_discount', 'brand', 'brand_type', 'category']\n",
    "\n",
    "data[train_cols] = data[train_cols].fillna(data[train_cols].mean())\n",
    "train = data[data['redemption_status'].notnull()]\n",
    "test = data[data['redemption_status'].isnull()]\n",
    "data = pd.concat([train, test], sort=False).reset_index(drop = True)\n",
    "ltr = len(train)\n",
    "def get_importances(clfs):\n",
    "    importances = [clf.feature_importance('gain') for clf in clfs]\n",
    "    importances = np.vstack(importances)\n",
    "    mean_gain = np.mean(importances, axis=0)\n",
    "    features = clfs[0].feature_name()\n",
    "    data = pd.DataFrame({'gain':mean_gain, 'feature':features})\n",
    "    plt.figure(figsize=(8, 30))\n",
    "    sns.barplot(x='gain', y='feature', data=data.sort_values('gain', ascending=False))\n",
    "    plt.tight_layout()\n",
    "    return data\n",
    "def standart_split(data, n_splits):\n",
    "    split_list = []\n",
    "    for i in range(n_splits):\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 228)\n",
    "        for train_index, test_index in kf.split(data.iloc[:ltr, :], data['redemption_status'][:ltr]):\n",
    "            split_list += [(train_index, test_index)]\n",
    "    return split_list\n",
    "\n",
    "split_list = standart_split(data, 1)\n",
    "\n",
    "def lgb_train(data, target, ltr, train_cols, split_list, param, n_e = 10000, cat_col = None, verb_num = None, imp=False):\n",
    "    pred = pd.DataFrame()\n",
    "    pred_val = np.zeros(ltr)\n",
    "    score = []\n",
    "    j = 0\n",
    "    train_pred = pd.DataFrame()\n",
    "    models = []\n",
    "    for i , (train_index, test_index) in enumerate(split_list):\n",
    "        param['seed'] = i\n",
    "        tr = lgb.Dataset(np.array(data[train_cols])[train_index], np.array(data[target])[train_index])\n",
    "        te = lgb.Dataset(np.array(data[train_cols])[test_index], np.array(data[target])[test_index], reference=tr)\n",
    "        tt = lgb.Dataset(np.array(data[train_cols])[ltr:, :])\n",
    "        evallist = [(tr, 'train'), (te, 'test')]\n",
    "        bst = lgb.train(param, tr, num_boost_round = n_e,valid_sets = [tr, te], feature_name=train_cols,\n",
    "                        early_stopping_rounds=150, verbose_eval = verb_num)\n",
    "        pred[str(i)] =bst.predict(np.array(data[train_cols])[ltr:])\n",
    "        pred_val[test_index] = bst.predict(np.array(data[train_cols])[test_index])\n",
    "        score += [metrics.roc_auc_score(np.array(data[target])[test_index], pred_val[test_index])]\n",
    "        models.append(bst)\n",
    "        print(i, 'MEAN: ', np.mean(score), 'LAST: ', score[-1])\n",
    "    if imp:\n",
    "        get_importances(models)\n",
    "        plt.show()\n",
    "    train_pred[str(j)] = pred_val\n",
    "    ans = pd.Series( pred.mean(axis = 1).tolist())\n",
    "    ans.name = 'lgb'\n",
    "    return pred, score, train_pred, bst\n",
    "\n",
    "param_lgb = { 'boosting_type': 'gbdt', 'objective': 'binary', 'metric':'auc',\n",
    "             'bagging_freq':1, 'subsample':1, 'feature_fraction': 0.7,\n",
    "              'num_leaves': 8, 'learning_rate': 0.05, 'lambda_l1':5,'max_bin':255}\n",
    "\n",
    "prediction, scores, oof, model = lgb_train(data, 'redemption_status', ltr, train_cols,\n",
    "                       split_list, param_lgb,  verb_num  = 250)\n",
    "\n",
    "tmp = prediction.copy()\n",
    "for col in tmp.columns:\n",
    "    tmp[col] = tmp[col].rank()\n",
    "tmp = tmp.mean(axis = 1)\n",
    "tmp  =tmp / tmp.max()\n",
    "day = 1\n",
    "sub = 2\n",
    "name = f\"day_{day}_sub_{sub}\"\n",
    "tmp = dict(zip(test.id.values, tmp))\n",
    "answer1 = pd.DataFrame()\n",
    "answer1['id'] = test.id.values\n",
    "answer1['redemption_status'] = answer1['id'].map(tmp)\n",
    "answer1.to_csv(f'{name}.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
